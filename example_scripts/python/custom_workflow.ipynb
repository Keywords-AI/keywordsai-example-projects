{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment V2 Custom Workflows - Jupyter Notebook\n",
    "\n",
    "This notebook demonstrates the complete workflow for creating and managing custom evaluation experiments using **Keywords AI Experiment V2 API**.\n",
    "\n",
    "## What are Custom Workflows?\n",
    "\n",
    "Custom workflows allow you to:\n",
    "- **Submit your own workflow results** via API\n",
    "- **Leverage automatic evaluator execution** on your outputs\n",
    "- **Process dataset entries** with your own models/logic\n",
    "- **Track and evaluate** custom processing pipelines\n",
    "\n",
    "## Workflow Overview\n",
    "\n",
    "1. **Create Experiment** - Initialize a custom workflow experiment (creates placeholder traces)\n",
    "2. **Get Summary** - View aggregated statistics for the experiment\n",
    "3. **List Traces** - Retrieve placeholder traces with dataset inputs\n",
    "4. **Get Trace Details** - Fetch full input content for processing\n",
    "5. **Process Input** - Run your custom logic on the input data\n",
    "6. **Submit Results** - Update traces with your workflow outputs\n",
    "7. **Check Evaluators** - View automatic evaluator scores\n",
    "\n",
    "## API Documentation\n",
    "\n",
    "- **Base URL**: `https://api.keywordsai.co`\n",
    "- **Experiment V2**: `/api/v2/experiments/`\n",
    "- **Authentication**: Bearer token in Authorization header\n",
    "\n",
    "## Requirements\n",
    "\n",
    "```bash\n",
    "pip install requests\n",
    "```\n",
    "\n",
    "## ‚ö†Ô∏è Known Issues & API Differences\n",
    "\n",
    "### Backend Issues:\n",
    "1. **Summary endpoint** (`/logs/summary/`) - May return `NaN` for `avg_latency` causing 500 errors\n",
    "2. **List endpoint** (`/logs/list/`) - May fail with 500 when filtering\n",
    "\n",
    "**Workarounds implemented:**\n",
    "- Summary calls are optional and will be skipped if they fail\n",
    "- List calls use GET without filters to avoid backend serialization bugs\n",
    "- Detailed error messages show what the backend returned\n",
    "\n",
    "### API Behavior vs Documentation:\n",
    "The actual API behavior differs from the provided documentation:\n",
    "\n",
    "| Documented Behavior | Actual Behavior |\n",
    "|---------------------|-----------------|\n",
    "| Traces have status `'pending_custom_submission'` | Traces have status `'success'` |\n",
    "| Need to filter for pending traces | Process all traces from custom workflows |\n",
    "| Placeholder output with specific message | Varies (check trace details) |\n",
    "\n",
    "**This notebook handles both documented and actual API behavior.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import json\n",
    "from typing import Dict, Any, List, Optional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "**‚ö†Ô∏è Important:** Replace the placeholder values below with your actual credentials and IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API Configuration\n",
    "BASE_URL = \"http://127.0.0.1:8000\"\n",
    "API_KEY = \"apikey\"  # Replace with your actual API key\n",
    "\n",
    "# Experiment Configuration\n",
    "EXPERIMENT_NAME = \"My Custom Workflow Experiment\"\n",
    "EXPERIMENT_DESCRIPTION = \"Testing custom workflow implementation with V2 API\"\n",
    "DATASET_ID = \"2cfbb5b4-53d2-4232-abd3-de4886f0255e\"  # Replace with your dataset ID\n",
    "\n",
    "# Workflow Configuration\n",
    "WORKFLOW_CONFIG = {\n",
    "    \"type\": \"custom\",\n",
    "    \"config\": {\n",
    "        \"name\": \"Custom Processing Workflow\",\n",
    "        \"description\": \"User-defined workflow for custom processing\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Evaluator Configuration (list of evaluator slugs)\n",
    "EVALUATOR_SLUGS = [\n",
    "    \"edb79105-0efe-42fd-b6b5-6e10d2b04117\",  # Replace with your evaluator slugs\n",
    "]\n",
    "\n",
    "# Global state variables (will be populated during execution)\n",
    "experiment_id = None\n",
    "trace_ids = []  # List of trace IDs to process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. API Functions\n",
    "\n",
    "These functions handle individual API operations for the experiment workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Create Custom Workflow Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_experiment(name: str, description: str, dataset_id: str, \n",
    "                     workflows: List[Dict], evaluator_slugs: List[str]) -> Dict[str, Any]:\n",
    "    \"\"\"Create a new custom workflow experiment.\n",
    "    \n",
    "    This creates placeholder traces for each dataset entry that you can\n",
    "    update with your custom workflow results.\n",
    "    \"\"\"\n",
    "    url = f\"{BASE_URL}/api/v2/experiments/\"\n",
    "    \n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": f\"Bearer {API_KEY}\"\n",
    "    }\n",
    "    \n",
    "    payload = {\n",
    "        \"name\": name,\n",
    "        \"description\": description,\n",
    "        \"dataset_id\": dataset_id,\n",
    "        \"workflows\": workflows,\n",
    "        \"evaluator_slugs\": evaluator_slugs\n",
    "    }\n",
    "    \n",
    "    print(\"Creating custom workflow experiment...\")\n",
    "    print(f\"  URL: {url}\")\n",
    "    print(f\"  Name: {name}\")\n",
    "    print(f\"  Dataset: {dataset_id}\")\n",
    "    print(f\"  Evaluators: {', '.join(evaluator_slugs)}\")\n",
    "    print(f\"  Request Body: {json.dumps(payload, indent=2)}\")\n",
    "    \n",
    "    response = requests.post(url, headers=headers, json=payload)\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    data = response.json()\n",
    "    print(f\"\\n‚úì Experiment created with ID: {data.get('id')}\")\n",
    "    print(f\"  Status: {data.get('status')}\")\n",
    "    print(\"  Placeholder traces are being created asynchronously...\")\n",
    "    \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Get Experiment Summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_experiment_summary(exp_id: str, filters: Optional[List[Dict]] = None) -> Dict[str, Any]:\n",
    "    \"\"\"Get aggregated summary statistics for experiment traces.\"\"\"\n",
    "    url = f\"{BASE_URL}/api/v2/experiments/{exp_id}/logs/summary/\"\n",
    "    \n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {API_KEY}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nGetting experiment summary for {exp_id}...\")\n",
    "    print(f\"  URL: {url}\")\n",
    "    \n",
    "    if filters:\n",
    "        request_body = {\"filters\": filters}\n",
    "        print(f\"  Method: POST\")\n",
    "        print(f\"  Request Body: {json.dumps(request_body, indent=2)}\")\n",
    "        response = requests.post(url, headers=headers, json=request_body)\n",
    "    else:\n",
    "        print(f\"  Method: GET\")\n",
    "        response = requests.get(url, headers=headers)\n",
    "    \n",
    "    response.raise_for_status()\n",
    "    \n",
    "    data = response.json()\n",
    "    print(f\"‚úì Summary retrieved:\")\n",
    "    print(f\"  Total traces: {data.get('total_count', 0)}\")\n",
    "    print(f\"  Total cost: ${data.get('total_cost', 0):.4f}\")\n",
    "    print(f\"  Total tokens: {data.get('total_tokens', 0)}\")\n",
    "    print(f\"  Avg latency: {data.get('avg_latency', 0):.2f}s\")\n",
    "    \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 List Experiment Logs (Traces)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_experiment_logs(exp_id: str, filters: Optional[List[Dict]] = None, \n",
    "                        page: int = 1, page_size: int = 100) -> Dict[str, Any]:\n",
    "    \"\"\"List traces for an experiment with optional filtering.\n",
    "    \n",
    "    For custom workflows, this returns placeholder traces containing the\n",
    "    dataset inputs you need to process.\n",
    "    \"\"\"\n",
    "    url = f\"{BASE_URL}/api/v2/experiments/{exp_id}/logs/list/\"\n",
    "    \n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {API_KEY}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    \n",
    "    params = {\n",
    "        \"page\": page,\n",
    "        \"page_size\": page_size\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nListing experiment logs for {exp_id}...\")\n",
    "    print(f\"  URL: {url}\")\n",
    "    print(f\"  Params: {params}\")\n",
    "    if filters:\n",
    "        print(f\"  Filters: {json.dumps(filters, indent=2)}\")\n",
    "    \n",
    "    # Try to get logs\n",
    "    try:\n",
    "        if filters:\n",
    "            request_body = {\"filters\": filters}\n",
    "            print(f\"  Method: POST\")\n",
    "            print(f\"  Request Body: {json.dumps(request_body, indent=2)}\")\n",
    "            response = requests.post(url, headers=headers, json=request_body, params=params)\n",
    "        else:\n",
    "            print(f\"  Method: GET\")\n",
    "            response = requests.get(url, headers=headers, params=params)\n",
    "        \n",
    "        response.raise_for_status()\n",
    "        \n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        print(f\"‚ùå Error listing logs: {e}\")\n",
    "        print(f\"   Status code: {e.response.status_code}\")\n",
    "        \n",
    "        # Show response content for debugging\n",
    "        try:\n",
    "            error_json = e.response.json()\n",
    "            print(f\"   Response: {error_json}\")\n",
    "        except:\n",
    "            print(f\"   Response (text): {e.response.text[:500]}\")\n",
    "        \n",
    "        # If POST with filters failed with 500, try GET without filters\n",
    "        if filters and e.response.status_code == 500:\n",
    "            print(\"\\n‚ö† Retrying without filters (backend may have serialization bug)...\")\n",
    "            try:\n",
    "                response = requests.get(url, headers=headers, params=params)\n",
    "                response.raise_for_status()\n",
    "                print(\"‚úì GET without filters succeeded\")\n",
    "            except Exception as retry_error:\n",
    "                print(f\"‚ùå Retry also failed: {retry_error}\")\n",
    "                raise e  # Raise original error\n",
    "        else:\n",
    "            raise\n",
    "    \n",
    "    data = response.json()\n",
    "    results = data.get('results', [])\n",
    "    print(f\"‚úì Found {len(results)} logs (page {page})\")\n",
    "    print(f\"  Total count: {data.get('count', 0)}\")\n",
    "    \n",
    "    if results:\n",
    "        status_counts = {}\n",
    "        for log in results:\n",
    "            status = log.get('status', 'unknown')\n",
    "            status_counts[status] = status_counts.get(status, 0) + 1\n",
    "        print(f\"  Status breakdown: {status_counts}\")\n",
    "    \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Get Trace Details\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trace_details(exp_id: str, trace_id: str, include_full_span_tree: bool = True) -> Dict[str, Any]:\n",
    "    \"\"\"Get detailed information about a specific trace.\n",
    "    \n",
    "    Use this to retrieve full input content (untruncated) for processing.\n",
    "    \n",
    "    Args:\n",
    "        exp_id: Experiment ID\n",
    "        trace_id: Trace ID\n",
    "        include_full_span_tree: If True, includes detail=1 to get full span tree with evaluator results\n",
    "    \"\"\"\n",
    "    url = f\"{BASE_URL}/api/v2/experiments/{exp_id}/logs/{trace_id}/\"\n",
    "    \n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {API_KEY}\"\n",
    "    }\n",
    "    \n",
    "    params = {}\n",
    "    if include_full_span_tree:\n",
    "        params[\"detail\"] = 1  # Include full span tree with evaluator results\n",
    "    \n",
    "    print(f\"  Getting trace details...\")\n",
    "    print(f\"    URL: {url}\")\n",
    "    print(f\"    Method: GET\")\n",
    "    if params:\n",
    "        print(f\"    Params: {params}\")\n",
    "    \n",
    "    response = requests.get(url, headers=headers, params=params)\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    return response.json()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Submit Custom Workflow Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def submit_workflow_results(exp_id: str, trace_id: str, \n",
    "                           input_data: Any, output_data: Any,\n",
    "                           name: Optional[str] = None, \n",
    "                           metadata: Optional[Dict] = None) -> Dict[str, Any]:\n",
    "    \"\"\"Update a placeholder trace with your custom workflow results.\n",
    "    \n",
    "    The system will automatically run configured evaluators on your output.\n",
    "    \"\"\"\n",
    "    url = f\"{BASE_URL}/api/v2/experiments/{exp_id}/logs/{trace_id}/\"\n",
    "    \n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {API_KEY}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    \n",
    "    payload = {\n",
    "        \"input\": input_data,\n",
    "        \"output\": output_data\n",
    "    }\n",
    "    \n",
    "    if name:\n",
    "        payload[\"name\"] = name\n",
    "    if metadata:\n",
    "        payload[\"metadata\"] = metadata\n",
    "    \n",
    "    print(f\"    Submitting results to: {url}\")\n",
    "    print(f\"    Method: PATCH\")\n",
    "    print(f\"    Payload keys: {list(payload.keys())}\")\n",
    "    # Don't print full input/output as they might be large\n",
    "    print(f\"    Input type: {type(input_data).__name__}\")\n",
    "    print(f\"    Output type: {type(output_data).__name__}\")\n",
    "    \n",
    "    response = requests.patch(url, headers=headers, json=payload)\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    updated_trace = response.json()\n",
    "    \n",
    "    # Validate the submission was successful\n",
    "    print(f\"    Response status: {response.status_code}\")\n",
    "    \n",
    "    # Check if the response contains expected fields\n",
    "    if 'id' not in updated_trace:\n",
    "        print(f\"    ‚ö† Warning: Response missing 'id' field\")\n",
    "    \n",
    "    # Check if status changed (optional, may not always apply)\n",
    "    response_status = updated_trace.get('status')\n",
    "    if response_status:\n",
    "        print(f\"    Trace status: {response_status}\")\n",
    "    \n",
    "    return updated_trace\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Validate Workflow Submission\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_workflow_submission(exp_id: str, trace_id: str, \n",
    "                                expected_output: Any, \n",
    "                                verify_by_refetch: bool = False) -> bool:\n",
    "    \"\"\"Validate that a workflow submission was successful.\n",
    "    \n",
    "    Args:\n",
    "        exp_id: Experiment ID\n",
    "        trace_id: Trace ID that was updated\n",
    "        expected_output: The output data that was submitted\n",
    "        verify_by_refetch: If True, re-fetch the trace to verify the update persisted\n",
    "        \n",
    "    Returns:\n",
    "        True if validation passes, False otherwise\n",
    "    \"\"\"\n",
    "    if not verify_by_refetch:\n",
    "        # Basic validation - just check if we got a trace ID back\n",
    "        return trace_id is not None\n",
    "    \n",
    "    # Advanced validation - re-fetch and compare\n",
    "    try:\n",
    "        print(f\"    Validating submission by re-fetching trace...\")\n",
    "        refetched_trace = get_trace_details(exp_id, trace_id, include_full_span_tree=False)\n",
    "        \n",
    "        # Check if output was updated\n",
    "        refetched_output = refetched_trace.get('output')\n",
    "        \n",
    "        # Convert both to strings for comparison (in case of type differences)\n",
    "        expected_str = json.dumps(expected_output, sort_keys=True) if not isinstance(expected_output, str) else expected_output\n",
    "        refetched_str = json.dumps(refetched_output, sort_keys=True) if not isinstance(refetched_output, str) else str(refetched_output)\n",
    "        \n",
    "        if expected_str == refetched_str:\n",
    "            print(f\"    ‚úì Validation passed: Output matches\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"    ‚ö† Validation warning: Output differs\")\n",
    "            print(f\"      Expected length: {len(expected_str)}\")\n",
    "            print(f\"      Refetched length: {len(refetched_str)}\")\n",
    "            # Not necessarily an error - API might transform the data\n",
    "            return True  # Return True but warn\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"    ‚ö† Validation error: {e}\")\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7 Wait Helper Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wait_for_processing(seconds: int = 10):\n",
    "    \"\"\"Wait for placeholder traces to be created or evaluators to run.\"\"\"\n",
    "    print(f\"\\nWaiting {seconds} seconds for processing...\")\n",
    "    time.sleep(seconds)\n",
    "    print(\"‚úì Wait complete\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Custom Processing Logic\n",
    "\n",
    "Define your custom workflow logic here. **Replace this placeholder function with your actual processing logic.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_with_custom_logic(input_data: Any) -> Dict[str, Any]:\n",
    "    \"\"\"Your custom workflow processing logic goes here.\n",
    "    \n",
    "    TODO: Replace this with your actual processing logic.\n",
    "    Examples:\n",
    "    - Call your own LLM API\n",
    "    - Run a fine-tuned model\n",
    "    - Execute a custom pipeline\n",
    "    - Combine multiple models\n",
    "    \"\"\"\n",
    "    # Parse input if it's a JSON string\n",
    "    if isinstance(input_data, str):\n",
    "        try:\n",
    "            parsed_input = json.loads(input_data)\n",
    "        except:\n",
    "            parsed_input = input_data\n",
    "    else:\n",
    "        parsed_input = input_data\n",
    "    \n",
    "    # Placeholder response - replace with your actual workflow\n",
    "    output = {\n",
    "        \"response\": \"This is a placeholder. Replace with your actual workflow output.\",\n",
    "        \"confidence\": 0.95,\n",
    "        \"model_used\": \"custom-model-v1\"\n",
    "    }\n",
    "    \n",
    "    metadata = {\n",
    "        \"processing_time\": 1.5,\n",
    "        \"tokens_used\": 150,\n",
    "        \"custom_metric\": 0.85\n",
    "    }\n",
    "    \n",
    "    return {\n",
    "        \"output\": output,\n",
    "        \"metadata\": metadata\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Main Workflow Execution\n",
    "\n",
    "This orchestrates the complete workflow from creation to evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_complete_workflow(max_traces: int = 5):\n",
    "    \"\"\"Execute the complete custom workflow from start to finish.\"\"\"\n",
    "    global experiment_id, trace_ids\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"CUSTOM WORKFLOW EXPERIMENT - COMPLETE EXECUTION\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    try:\n",
    "        # Step 1: Create experiment\n",
    "        print(\"\\n[STEP 1] Creating custom workflow experiment...\")\n",
    "        experiment_data = create_experiment(\n",
    "            name=EXPERIMENT_NAME,\n",
    "            description=EXPERIMENT_DESCRIPTION,\n",
    "            dataset_id=DATASET_ID,\n",
    "            workflows=[WORKFLOW_CONFIG],\n",
    "            evaluator_slugs=EVALUATOR_SLUGS\n",
    "        )\n",
    "        experiment_id = experiment_data.get('id')\n",
    "        \n",
    "        # Step 2: Wait for placeholder traces\n",
    "        print(\"\\n[STEP 2] Waiting for placeholder traces to be created...\")\n",
    "        print(\"  (Traces are created asynchronously, this may take 15-30 seconds)\")\n",
    "        wait_for_processing(20)\n",
    "        \n",
    "        # Step 3: Get experiment summary (optional - may fail if traces not ready)\n",
    "        print(\"\\n[STEP 3] Getting experiment summary...\")\n",
    "        try:\n",
    "            summary = get_experiment_summary(experiment_id)\n",
    "        except requests.exceptions.HTTPError as e:\n",
    "            if e.response.status_code == 500:\n",
    "                print(\"‚ö† Summary endpoint returned 500 (backend bug with NaN values). Skipping summary check.\")\n",
    "                summary = None\n",
    "            else:\n",
    "                raise\n",
    "        \n",
    "        # Step 4: List placeholder traces (no filter to see all traces)\n",
    "        print(\"\\n[STEP 4] Listing placeholder traces...\")\n",
    "        logs_data = list_experiment_logs(experiment_id, filters=None, page_size=max_traces)\n",
    "        \n",
    "        results = logs_data.get('results', [])\n",
    "        if not results:\n",
    "            print(\"‚ö† No traces found yet. Placeholder traces are still being created.\")\n",
    "            print(\"   Try waiting longer or running the workflow again in a few moments.\")\n",
    "            return None\n",
    "        \n",
    "        print(f\"‚úì Found {len(results)} trace(s)\")\n",
    "        \n",
    "        # Show first trace structure for debugging\n",
    "        if results:\n",
    "            print(f\"\\n  üìã First trace structure (for debugging):\")\n",
    "            first_trace = results[0]\n",
    "            print(f\"    Keys: {list(first_trace.keys())}\")\n",
    "            print(f\"    ID: {first_trace.get('id')}\")\n",
    "            print(f\"    Status: {first_trace.get('status')}\")\n",
    "            print(f\"    Name: {first_trace.get('name')}\")\n",
    "            if 'input' in first_trace:\n",
    "                input_preview = str(first_trace.get('input'))[:100]\n",
    "                print(f\"    Input preview: {input_preview}...\")\n",
    "            if 'output' in first_trace:\n",
    "                output_preview = str(first_trace.get('output'))[:100]\n",
    "                print(f\"    Output preview: {output_preview}...\")\n",
    "        \n",
    "\n",
    "   \n",
    "        print(f\"\\n  ‚ÑπÔ∏è  Processing all traces (API docs may be outdated)\")\n",
    "        status_counts = {}\n",
    "        for t in results:\n",
    "            status = t.get('status', 'unknown')\n",
    "            status_counts[status] = status_counts.get(status, 0) + 1\n",
    "        print(f\"  Status breakdown: {status_counts}\")\n",
    "        \n",
    "        results = results[:max_traces]  # Limit to max_traces\n",
    "        print(f\"  ‚Üí Will process {len(results)} trace(s)\")\n",
    "        \n",
    "        # Step 5 & 6: Process and submit results\n",
    "        print(f\"\\n[STEP 5-6] Processing and submitting {len(results)} traces...\")\n",
    "        processed_traces = []\n",
    "        \n",
    "        for i, trace in enumerate(results, 1):\n",
    "            trace_id = trace.get('id')\n",
    "            trace_ids.append(trace_id)\n",
    "            \n",
    "            print(f\"\\n  Processing trace {i}/{len(results)} ({trace_id[:16]}...)...\")\n",
    "            \n",
    "            # Get trace details (without full span tree to keep it fast)\n",
    "            trace_details = get_trace_details(experiment_id, trace_id, include_full_span_tree=False)\n",
    "            \n",
    "            # Extract input from span_tree or top level\n",
    "            span_tree = trace_details.get('span_tree', [])\n",
    "            input_data = span_tree[0].get('input') if span_tree else trace_details.get('input')\n",
    "            \n",
    "            print(f\"    Input preview: {str(input_data)[:100]}...\")\n",
    "            \n",
    "            # Process with custom logic\n",
    "            print(f\"    Running custom workflow...\")\n",
    "            result = process_with_custom_logic(input_data)\n",
    "            \n",
    "            # Submit results\n",
    "            print(f\"    Submitting results...\")\n",
    "            updated_trace = submit_workflow_results(\n",
    "                exp_id=experiment_id,\n",
    "                trace_id=trace_id,\n",
    "                input_data=input_data,\n",
    "                output_data=result['output'],\n",
    "                name=f\"Processed: {trace.get('name', 'Trace')}\",\n",
    "                metadata=result.get('metadata')\n",
    "            )\n",
    "            \n",
    "            # Validate submission\n",
    "            is_valid = validate_workflow_submission(\n",
    "                exp_id=experiment_id,\n",
    "                trace_id=trace_id,\n",
    "                expected_output=result['output'],\n",
    "                verify_by_refetch=False  # Set to True for thorough validation (slower)\n",
    "            )\n",
    "            \n",
    "            if is_valid:\n",
    "                processed_traces.append(updated_trace)\n",
    "                print(f\"    ‚úì Submitted and validated successfully\")\n",
    "            else:\n",
    "                print(f\"    ‚ö† Submission validation failed for trace {trace_id}\")\n",
    "        \n",
    "        print(f\"\\n‚úì Successfully processed and submitted {len(processed_traces)} traces\")\n",
    "        \n",
    "        # Step 7: Wait for evaluators\n",
    "        print(\"\\n[STEP 7] Waiting for evaluators to complete...\")\n",
    "        wait_for_processing(15)\n",
    "        \n",
    "        # Step 8: Get final summary (optional)\n",
    "        print(\"\\n[STEP 8] Getting final experiment summary...\")\n",
    "        try:\n",
    "            final_summary = get_experiment_summary(experiment_id)\n",
    "        except requests.exceptions.HTTPError as e:\n",
    "            if e.response.status_code == 500:\n",
    "                print(\"‚ö† Summary endpoint returned 500 (backend bug with NaN values). Skipping final summary.\")\n",
    "                final_summary = {}\n",
    "            else:\n",
    "                raise\n",
    "        \n",
    "        # Display results\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"WORKFLOW COMPLETED SUCCESSFULLY\")\n",
    "        print(\"=\" * 70)\n",
    "        print(f\"Experiment ID: {experiment_id}\")\n",
    "        print(f\"Traces Processed: {len(processed_traces)}\")\n",
    "        print(f\"Total Cost: ${final_summary.get('total_cost', 0):.4f}\")\n",
    "        print(f\"Total Tokens: {final_summary.get('total_tokens', 0)}\")\n",
    "        \n",
    "        return {\n",
    "            \"experiment_id\": experiment_id,\n",
    "            \"processed_count\": len(processed_traces),\n",
    "            \"trace_ids\": trace_ids,\n",
    "            \"summary\": final_summary\n",
    "        }\n",
    "        \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"\\n‚ùå Error: {e}\")\n",
    "        if hasattr(e, 'response') and hasattr(e.response, 'text'):\n",
    "            print(f\"Response: {e.response.text}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Unexpected error: {e}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Run the Workflow\n",
    "\n",
    "Execute the complete workflow. **Make sure you've configured Section 2 first!**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "CUSTOM WORKFLOW EXPERIMENT - COMPLETE EXECUTION\n",
      "======================================================================\n",
      "\n",
      "[STEP 1] Creating custom workflow experiment...\n",
      "Creating custom workflow experiment...\n",
      "  URL: http://127.0.0.1:8000/api/v2/experiments/\n",
      "  Name: My Custom Workflow Experiment\n",
      "  Dataset: 2cfbb5b4-53d2-4232-abd3-de4886f0255e\n",
      "  Evaluators: edb79105-0efe-42fd-b6b5-6e10d2b04117\n",
      "  Request Body: {\n",
      "  \"name\": \"My Custom Workflow Experiment\",\n",
      "  \"description\": \"Testing custom workflow implementation with V2 API\",\n",
      "  \"dataset_id\": \"2cfbb5b4-53d2-4232-abd3-de4886f0255e\",\n",
      "  \"workflows\": [\n",
      "    {\n",
      "      \"type\": \"custom\",\n",
      "      \"config\": {\n",
      "        \"name\": \"Custom Processing Workflow\",\n",
      "        \"description\": \"User-defined workflow for custom processing\"\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"evaluator_slugs\": [\n",
      "    \"edb79105-0efe-42fd-b6b5-6e10d2b04117\"\n",
      "  ]\n",
      "}\n",
      "\n",
      "‚úì Experiment created with ID: 1eac635fc8c848c19d93a72c7622cddd\n",
      "  Status: pending\n",
      "  Placeholder traces are being created asynchronously...\n",
      "\n",
      "[STEP 2] Waiting for placeholder traces to be created...\n",
      "  (Traces are created asynchronously, this may take 15-30 seconds)\n",
      "\n",
      "Waiting 20 seconds for processing...\n",
      "‚úì Wait complete\n",
      "\n",
      "[STEP 3] Getting experiment summary...\n",
      "\n",
      "Getting experiment summary for 1eac635fc8c848c19d93a72c7622cddd...\n",
      "  URL: http://127.0.0.1:8000/api/v2/experiments/1eac635fc8c848c19d93a72c7622cddd/logs/summary/\n",
      "  Method: GET\n",
      "‚úì Summary retrieved:\n",
      "  Total traces: 3\n",
      "  Total cost: $0.0000\n",
      "  Total tokens: 0\n",
      "  Avg latency: 10.88s\n",
      "\n",
      "[STEP 4] Listing placeholder traces...\n",
      "\n",
      "Listing experiment logs for 1eac635fc8c848c19d93a72c7622cddd...\n",
      "  URL: http://127.0.0.1:8000/api/v2/experiments/1eac635fc8c848c19d93a72c7622cddd/logs/list/\n",
      "  Params: {'page': 1, 'page_size': 5}\n",
      "  Method: GET\n",
      "‚úì Found 3 logs (page 1)\n",
      "  Total count: 3\n",
      "  Status breakdown: {'success': 3}\n",
      "‚úì Found 3 trace(s)\n",
      "\n",
      "  üìã First trace structure (for debugging):\n",
      "    Keys: ['id', 'trace_unique_id', 'root_span_unique_id', 'unique_organization_id', 'environment', 'customer_identifier', 'start_time', 'end_time', 'duration', 'span_count', 'llm_call_count', 'total_cost', 'total_prompt_tokens', 'total_completion_tokens', 'total_tokens', 'error_count', 'name', 'input', 'output', 'storage_object_key', 'comparison_key', 'status', 'updated_storage_object_key', 'unique_id']\n",
      "    ID: 436c34e878584a449da861600f5b05d0\n",
      "    Status: success\n",
      "    Name: experiment_trace\n",
      "    Input preview: [{\"role\": \"system\", \"content\": [{\"type\": \"text\", \"...\n",
      "    Output preview: ...\n",
      "\n",
      "  ‚ÑπÔ∏è  Processing all traces (API docs may be outdated)\n",
      "  Status breakdown: {'success': 3}\n",
      "  ‚Üí Will process 3 trace(s)\n",
      "\n",
      "[STEP 5-6] Processing and submitting 3 traces...\n",
      "\n",
      "  Processing trace 1/3 (436c34e878584a44...)...\n",
      "  Getting trace details...\n",
      "    URL: http://127.0.0.1:8000/api/v2/experiments/1eac635fc8c848c19d93a72c7622cddd/logs/436c34e878584a449da861600f5b05d0/\n",
      "    Method: GET\n",
      "    Input preview: [{\"role\": \"system\", \"content\": [{\"type\": \"text\", \"...\n",
      "    Running custom workflow...\n",
      "    Submitting results...\n",
      "    Submitting results to: http://127.0.0.1:8000/api/v2/experiments/1eac635fc8c848c19d93a72c7622cddd/logs/436c34e878584a449da861600f5b05d0/\n",
      "    Method: PATCH\n",
      "    Payload keys: ['input', 'output', 'name', 'metadata']\n",
      "    Input type: str\n",
      "    Output type: dict\n",
      "    Response status: 200\n",
      "    Trace status: success\n",
      "    ‚úì Submitted and validated successfully\n",
      "\n",
      "  Processing trace 2/3 (53c74a30618f4536...)...\n",
      "  Getting trace details...\n",
      "    URL: http://127.0.0.1:8000/api/v2/experiments/1eac635fc8c848c19d93a72c7622cddd/logs/53c74a30618f4536a319feb2407b6496/\n",
      "    Method: GET\n",
      "    Input preview: [{\"role\": \"system\", \"content\": [{\"type\": \"text\", \"...\n",
      "    Running custom workflow...\n",
      "    Submitting results...\n",
      "    Submitting results to: http://127.0.0.1:8000/api/v2/experiments/1eac635fc8c848c19d93a72c7622cddd/logs/53c74a30618f4536a319feb2407b6496/\n",
      "    Method: PATCH\n",
      "    Payload keys: ['input', 'output', 'name', 'metadata']\n",
      "    Input type: str\n",
      "    Output type: dict\n",
      "    Response status: 200\n",
      "    Trace status: success\n",
      "    ‚úì Submitted and validated successfully\n",
      "\n",
      "  Processing trace 3/3 (bde70a9189b341f3...)...\n",
      "  Getting trace details...\n",
      "    URL: http://127.0.0.1:8000/api/v2/experiments/1eac635fc8c848c19d93a72c7622cddd/logs/bde70a9189b341f382f6af6e370ef944/\n",
      "    Method: GET\n",
      "    Input preview: [{\"role\": \"system\", \"content\": [{\"type\": \"text\", \"...\n",
      "    Running custom workflow...\n",
      "    Submitting results...\n",
      "    Submitting results to: http://127.0.0.1:8000/api/v2/experiments/1eac635fc8c848c19d93a72c7622cddd/logs/bde70a9189b341f382f6af6e370ef944/\n",
      "    Method: PATCH\n",
      "    Payload keys: ['input', 'output', 'name', 'metadata']\n",
      "    Input type: str\n",
      "    Output type: dict\n",
      "    Response status: 200\n",
      "    Trace status: success\n",
      "    ‚úì Submitted and validated successfully\n",
      "\n",
      "‚úì Successfully processed and submitted 3 traces\n",
      "\n",
      "[STEP 7] Waiting for evaluators to complete...\n",
      "\n",
      "Waiting 15 seconds for processing...\n",
      "‚úì Wait complete\n",
      "\n",
      "[STEP 8] Getting final experiment summary...\n",
      "\n",
      "Getting experiment summary for 1eac635fc8c848c19d93a72c7622cddd...\n",
      "  URL: http://127.0.0.1:8000/api/v2/experiments/1eac635fc8c848c19d93a72c7622cddd/logs/summary/\n",
      "  Method: GET\n",
      "‚úì Summary retrieved:\n",
      "  Total traces: 3\n",
      "  Total cost: $0.0000\n",
      "  Total tokens: 0\n",
      "  Avg latency: 47.78s\n",
      "\n",
      "======================================================================\n",
      "WORKFLOW COMPLETED SUCCESSFULLY\n",
      "======================================================================\n",
      "Experiment ID: 1eac635fc8c848c19d93a72c7622cddd\n",
      "Traces Processed: 3\n",
      "Total Cost: $0.0000\n",
      "Total Tokens: 0\n"
     ]
    }
   ],
   "source": [
    "# Run the complete workflow\n",
    "# Process maximum 5 traces (change this number as needed)\n",
    "workflow_results = run_complete_workflow(max_traces=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Validate Workflow Submissions\n",
    "\n",
    "Verify that all workflow submissions were successful and data persisted correctly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "VALIDATING WORKFLOW SUBMISSIONS\n",
      "======================================================================\n",
      "\n",
      "Re-fetching 3 traces to verify submissions...\n",
      "\n",
      "[1/3] Validating trace 436c34e878584a44...\n",
      "  Getting trace details...\n",
      "    URL: http://127.0.0.1:8000/api/v2/experiments/1eac635fc8c848c19d93a72c7622cddd/logs/436c34e878584a449da861600f5b05d0/\n",
      "    Method: GET\n",
      "  ‚úì Valid - Status: success, Output present: 50 chars\n",
      "\n",
      "[2/3] Validating trace 53c74a30618f4536...\n",
      "  Getting trace details...\n",
      "    URL: http://127.0.0.1:8000/api/v2/experiments/1eac635fc8c848c19d93a72c7622cddd/logs/53c74a30618f4536a319feb2407b6496/\n",
      "    Method: GET\n",
      "  ‚úì Valid - Status: success, Output present: 50 chars\n",
      "\n",
      "[3/3] Validating trace bde70a9189b341f3...\n",
      "  Getting trace details...\n",
      "    URL: http://127.0.0.1:8000/api/v2/experiments/1eac635fc8c848c19d93a72c7622cddd/logs/bde70a9189b341f382f6af6e370ef944/\n",
      "    Method: GET\n",
      "  ‚úì Valid - Status: success, Output present: 50 chars\n",
      "\n",
      "======================================================================\n",
      "VALIDATION SUMMARY\n",
      "======================================================================\n",
      "Total traces: 3\n",
      "Valid submissions: 3\n",
      "Failed validations: 0\n",
      "\n",
      "‚úÖ All workflow submissions validated successfully!\n"
     ]
    }
   ],
   "source": [
    "# Validate all submitted traces\n",
    "if workflow_results and trace_ids:\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"VALIDATING WORKFLOW SUBMISSIONS\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    print(f\"\\nRe-fetching {len(trace_ids)} traces to verify submissions...\")\n",
    "    \n",
    "    validation_results = []\n",
    "    for i, trace_id in enumerate(trace_ids, 1):\n",
    "        print(f\"\\n[{i}/{len(trace_ids)}] Validating trace {trace_id[:16]}...\")\n",
    "        \n",
    "        try:\n",
    "            # Re-fetch the trace\n",
    "            trace = get_trace_details(experiment_id, trace_id, include_full_span_tree=False)\n",
    "            \n",
    "            # Check key fields\n",
    "            has_id = 'id' in trace\n",
    "            has_output = 'output' in trace and trace['output']\n",
    "            status = trace.get('status', 'unknown')\n",
    "            \n",
    "            validation_results.append({\n",
    "                'trace_id': trace_id,\n",
    "                'has_id': has_id,\n",
    "                'has_output': has_output,\n",
    "                'status': status,\n",
    "                'valid': has_id and has_output\n",
    "            })\n",
    "            \n",
    "            if has_id and has_output:\n",
    "                print(f\"  ‚úì Valid - Status: {status}, Output present: {len(str(trace['output']))} chars\")\n",
    "            else:\n",
    "                print(f\"  ‚ö† Issues detected:\")\n",
    "                if not has_id:\n",
    "                    print(f\"    - Missing ID\")\n",
    "                if not has_output:\n",
    "                    print(f\"    - Missing or empty output\")\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ùå Error validating: {e}\")\n",
    "            validation_results.append({\n",
    "                'trace_id': trace_id,\n",
    "                'valid': False,\n",
    "                'error': str(e)\n",
    "            })\n",
    "    \n",
    "    # Summary\n",
    "    valid_count = sum(1 for r in validation_results if r.get('valid', False))\n",
    "    print(f\"\\n\" + \"=\" * 70)\n",
    "    print(f\"VALIDATION SUMMARY\")\n",
    "    print(f\"=\" * 70)\n",
    "    print(f\"Total traces: {len(trace_ids)}\")\n",
    "    print(f\"Valid submissions: {valid_count}\")\n",
    "    print(f\"Failed validations: {len(trace_ids) - valid_count}\")\n",
    "    \n",
    "    if valid_count == len(trace_ids):\n",
    "        print(f\"\\n‚úÖ All workflow submissions validated successfully!\")\n",
    "    else:\n",
    "        print(f\"\\n‚ö† Some submissions may have issues - review details above\")\n",
    "    \n",
    "else:\n",
    "    print(\"\\n‚ö† No workflow results to validate. Run the workflow first.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. View Evaluator Results\n",
    "\n",
    "Check the evaluator scores for your submitted results.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python-ddkUAegS-py3.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
