{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3dad624",
   "metadata": {},
   "source": [
    "# KeywordsAI Multi-Modal Tool Evaluation Workflow Demo\n",
    "\n",
    "This notebook demonstrates the complete workflow for evaluating LLM agents with tool calls using KeywordsAI.\n",
    "\n",
    "## Overview\n",
    "1. **Agent Demo**: Run travel assistant with multi-modal inputs\n",
    "2. **Log Management**: Fetch and analyze logs\n",
    "3. **Evaluator Creation**: Create custom LLM evaluators  \n",
    "4. **Testset Management**: Create testsets from logs\n",
    "5. **Experiment Execution**: Compare prompt versions\n",
    "6. **Results Analysis**: Evaluate tool call accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d696d93b",
   "metadata": {},
   "source": [
    "## Step 0: Create the prompt for the agent\n",
    "go to src/example_workflows/multi_modal_tool_evals/prompts/traveling_agent_prompt.md and copy the prompt into the UI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a024357",
   "metadata": {},
   "source": [
    "## Step 1: Run Agent Demo\n",
    "\n",
    "```\n",
    "cd src/example_workflows/multi_modal_tool_evals\n",
    "python3 agent.py\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9cb072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è  No logs found - using demo data for the workflow\n"
     ]
    }
   ],
   "source": [
    "# Fetch logs from the last day filtered by evaluation identifier\n",
    "from example_workflows.multi_modal_tool_evals.logs import get_logs\n",
    "from datetime import datetime, timedelta\n",
    "from example_workflows.multi_modal_tool_evals.constants import EVALUATION_IDENTIFIER\n",
    "\n",
    "logs = get_logs(\n",
    "    start_time=datetime.now() - timedelta(days=1),\n",
    "    end_time=datetime.now(),\n",
    "    filters={\n",
    "        \"evaluation_identifier\": {\n",
    "            \"value\": EVALUATION_IDENTIFIER,\n",
    "        },\n",
    "    },\n",
    ")\n",
    "\n",
    "if logs and logs.get(\"results\"):\n",
    "    print(f\"üìä Found {len(logs['results'])} logs\")\n",
    "\n",
    "    # Analyze first log structure\n",
    "    first_log = logs[\"results\"][0]\n",
    "    print(f\"\\nüîç Sample log structure:\")\n",
    "    print(f\"- ID: {first_log['id']}\")\n",
    "    print(f\"- Customer ID: {first_log['keywordsai_params'].get('customer_identifier')}\")\n",
    "    print(\n",
    "        f\"- Variables: {list(first_log['keywordsai_params'].get('variables', {}).keys())}\"\n",
    "    )\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No logs found - using demo data for the workflow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650c9d74",
   "metadata": {},
   "source": [
    "## Step 3: Create Custom Evaluator\n",
    "\n",
    "Create a tool call accuracy evaluator to assess how well the agent uses tools.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d33d26b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Evaluator created successfully!\n",
      "Evaluator ID: 854bb98c-4529-490d-849a-f5d83054f314\n",
      "Evaluator slug: tool_call_accuracy_demo\n"
     ]
    }
   ],
   "source": [
    "# Create tool call accuracy evaluator\n",
    "evaluator_data = create_llm_evaluator(\n",
    "    evaluator_slug='tool_call_accuracy_demo',\n",
    "    name='Tool Call Accuracy Evaluator (Demo)',\n",
    "    evaluator_definition='Evaluate whether the AI agent correctly identified the need for tool calls and called the appropriate tools based on user input and context.',\n",
    "    scoring_rubric='''\n",
    "    Score 1.0: Perfect tool usage - called all necessary tools with correct parameters, no unnecessary calls\n",
    "    Score 0.8: Good tool usage - called most necessary tools correctly, minor parameter issues or one unnecessary call\n",
    "    Score 0.6: Adequate tool usage - called some necessary tools but missed important ones or had parameter errors\n",
    "    Score 0.4: Poor tool usage - called wrong tools or missed most necessary tool calls\n",
    "    Score 0.2: Very poor - called completely inappropriate tools or failed to call any necessary tools\n",
    "    Score 0.0: No tool calls when tools were clearly needed, or completely wrong tool usage\n",
    "    \n",
    "    Consider:\n",
    "    - Did the agent call search_places when user specified a travel category?\n",
    "    - Did the agent call check_weather when user requested weather information?\n",
    "    - Did the agent call find_hotels when user wanted hotel booking?\n",
    "    - Were the tool parameters (location, category) correct?\n",
    "    ''',\n",
    "    description='Evaluates the accuracy and appropriateness of tool calls made by the travel assistant agent',\n",
    "    min_score=0.0,\n",
    "    max_score=1.0,\n",
    "    passing_score=0.7\n",
    ")\n",
    "\n",
    "if evaluator_data:\n",
    "    print('‚úÖ Evaluator created successfully!')\n",
    "    print(f'Evaluator ID: {evaluator_data.get(\"id\")}')\n",
    "    print(f'Evaluator slug: {evaluator_data.get(\"evaluator_slug\")}')\n",
    "else:\n",
    "    print('‚ùå Failed to create evaluator - may already exist')\n",
    "    \n",
    "    # List existing evaluators\n",
    "    existing_evaluators = list_evaluators()\n",
    "    if existing_evaluators:\n",
    "        print(\"\\nüìã Existing evaluators:\")\n",
    "        for evaluator in existing_evaluators.get('results', []):\n",
    "            if 'tool_call_accuracy' in evaluator.get('evaluator_slug', ''):\n",
    "                print(f\"- {evaluator['name']} ({evaluator['evaluator_slug']})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efea9d0d",
   "metadata": {},
   "source": [
    "## Step 4: Create Testset\n",
    "\n",
    "Create a testset with test scenarios for the travel agent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19eb02d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Testset created: Travel Agent Multi-Modal Testset (Demo) (ID: 24501fc9ac1245d5b7042ffb8175b339)\n",
      "‚úÖ Added 2 test scenarios to testset\n"
     ]
    }
   ],
   "source": [
    "# Create testset for travel agent evaluation\n",
    "testset = create_testset(\n",
    "    name=\"Travel Agent Multi-Modal Testset (Demo)\",\n",
    "    description=\"Test dataset for travel agent with multi-modal inputs and tool calls\",\n",
    "    column_definitions=[\n",
    "        {\"field\": \"category\"},\n",
    "        {\"field\": \"name\"}, \n",
    "        {\"field\": \"is_booking_hotel\"},\n",
    "        {\"field\": \"is_checking_weather\"},\n",
    "        {\"field\": \"has_image\"},\n",
    "        {\"field\": \"expected_tools\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "if testset:\n",
    "    print(f\"‚úÖ Testset created: {testset['name']} (ID: {testset['id']})\")\n",
    "    \n",
    "    # Add sample test scenarios\n",
    "    sample_rows = [\n",
    "        {\n",
    "            'row_data': {\n",
    "                'category': 'beach',\n",
    "                'name': 'Mike (Beach Lover)',\n",
    "                'is_booking_hotel': False,\n",
    "                'is_checking_weather': False,\n",
    "                'has_image': True,\n",
    "                'expected_tools': 'search_places'\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            'row_data': {\n",
    "                'category': 'mountain',\n",
    "                'name': 'Sarah (Adventure Seeker)', \n",
    "                'is_booking_hotel': True,\n",
    "                'is_checking_weather': True,\n",
    "                'has_image': True,\n",
    "                'expected_tools': 'search_places,find_hotels,check_weather'\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    rows_result = create_testset_rows(testset['id'], sample_rows)\n",
    "    if rows_result:\n",
    "        print(f\"‚úÖ Added {len(sample_rows)} test scenarios to testset\")\n",
    "    else:\n",
    "        print(\"‚ùå Failed to add test scenarios\")\n",
    "else:\n",
    "    print(\"‚ùå Failed to create testset\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e8d8ef",
   "metadata": {},
   "source": [
    "## Step 5: Create Experiment with Prompts API\n",
    "\n",
    "Create an experiment using the prompts API to fetch actual prompt definitions rather than hardcoding them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6ccf445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã Found travel prompt: Travel Agent Demo Prompt\n",
      "Error: 400 Client Error: Bad Request for url: http://localhost:8000/api/experiments/create\n",
      "Response: {'columns': [\"Invalid column: [{'type': 'missing', 'loc': ('columns', 0, 'max_completion_tokens'), 'msg': 'Field required', 'input': {'id': 'ce02385ae81b4360ae0fa31e0cc9aa65', 'model': 'gpt-4o', 'name': 'Travel Agent v1', 'prompt_id': 'ce02385ae81b4360ae0fa31e0cc9aa65', 'prompt_version': 1, 'temperature': 0.7, 'tools': [{'type': 'function', 'function': {'name': 'search_places', 'description': 'Search for places based on landscape category', 'parameters': {'type': 'object', 'properties': {'category': {'type': 'string'}}, 'required': ['category']}}}]}}, {'type': 'missing', 'loc': ('columns', 0, 'top_p'), 'msg': 'Field required', 'input': {'id': 'ce02385ae81b4360ae0fa31e0cc9aa65', 'model': 'gpt-4o', 'name': 'Travel Agent v1', 'prompt_id': 'ce02385ae81b4360ae0fa31e0cc9aa65', 'prompt_version': 1, 'temperature': 0.7, 'tools': [{'type': 'function', 'function': {'name': 'search_places', 'description': 'Search for places based on landscape category', 'parameters': {'type': 'object', 'properties': {'category': {'type': 'string'}}, 'required': ['category']}}}]}}, {'type': 'missing', 'loc': ('columns', 0, 'frequency_penalty'), 'msg': 'Field required', 'input': {'id': 'ce02385ae81b4360ae0fa31e0cc9aa65', 'model': 'gpt-4o', 'name': 'Travel Agent v1', 'prompt_id': 'ce02385ae81b4360ae0fa31e0cc9aa65', 'prompt_version': 1, 'temperature': 0.7, 'tools': [{'type': 'function', 'function': {'name': 'search_places', 'description': 'Search for places based on landscape category', 'parameters': {'type': 'object', 'properties': {'category': {'type': 'string'}}, 'required': ['category']}}}]}}, {'type': 'missing', 'loc': ('columns', 0, 'presence_penalty'), 'msg': 'Field required', 'input': {'id': 'ce02385ae81b4360ae0fa31e0cc9aa65', 'model': 'gpt-4o', 'name': 'Travel Agent v1', 'prompt_id': 'ce02385ae81b4360ae0fa31e0cc9aa65', 'prompt_version': 1, 'temperature': 0.7, 'tools': [{'type': 'function', 'function': {'name': 'search_places', 'description': 'Search for places based on landscape category', 'parameters': {'type': 'object', 'properties': {'category': {'type': 'string'}}, 'required': ['category']}}}]}}]\"]}\n",
      "‚ùå Failed to create experiment\n"
     ]
    }
   ],
   "source": [
    "# Import prompts utilities\n",
    "from example_workflows.multi_modal_tool_evals.prompts import list_prompts, get_prompt\n",
    "\n",
    "# Find existing travel agent prompts or create new ones\n",
    "prompts = list_prompts()\n",
    "travel_prompt = None\n",
    "\n",
    "if prompts and prompts.get('results'):\n",
    "    for prompt in prompts['results']:\n",
    "        if 'travel' in prompt['name'].lower():\n",
    "            travel_prompt = prompt\n",
    "            break\n",
    "\n",
    "if travel_prompt:\n",
    "    print(f\"üìã Found travel prompt: {travel_prompt['name']}\")\n",
    "    prompt_details = get_prompt(travel_prompt['prompt_id'])\n",
    "    \n",
    "    # Create experiment using the prompt ID\n",
    "    experiment = create_experiment(\n",
    "        name=\"Travel Agent Evaluation (Prompt-based)\",\n",
    "        description=\"Comparing travel agent prompt versions using prompts API\",\n",
    "        columns=[\n",
    "            {\n",
    "                \"id\": prompt_details['prompt_id'],\n",
    "                \"model\": \"gpt-4o\",\n",
    "                \"name\": \"Travel Agent v1\",\n",
    "                \"prompt_id\": prompt_details['prompt_id'],\n",
    "                \"prompt_version\": 1,\n",
    "                \"temperature\": 0.7,\n",
    "                \"tools\": [\n",
    "                    {\n",
    "                        \"type\": \"function\",\n",
    "                        \"function\": {\n",
    "                            \"name\": \"search_places\",\n",
    "                            \"description\": \"Search for places based on landscape category\",\n",
    "                            \"parameters\": {\n",
    "                                \"type\": \"object\",\n",
    "                                \"properties\": {\"category\": {\"type\": \"string\"}},\n",
    "                                \"required\": [\"category\"]\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ],\n",
    "        rows=[\n",
    "            {\"input\": {\"category\": \"beach\", \"name\": \"Mike\", \"is_booking_hotel\": False}},\n",
    "            {\"input\": {\"category\": \"mountain\", \"name\": \"Sarah\", \"is_booking_hotel\": True}}\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    if experiment:\n",
    "        print(f\"‚úÖ Experiment created: {experiment['name']}\")\n",
    "        print(f\"ID: {experiment['id']}\")\n",
    "        print(f\"Using prompt ID: {prompt_details['prompt_id']}\")\n",
    "    else:\n",
    "        print(\"‚ùå Failed to create experiment\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No travel agent prompt found - would need to create one first\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060f4bd1",
   "metadata": {},
   "source": [
    "## Step 6: Run Experiment and Evaluation\n",
    "\n",
    "Execute the experiment and apply the evaluator to get tool call accuracy scores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54419c14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è No experiment to run - create one first\n"
     ]
    }
   ],
   "source": [
    "# Run the experiment (if we have one)\n",
    "if 'experiment' in locals() and experiment:\n",
    "    experiment_id = experiment['id']\n",
    "    \n",
    "    print(f\"üöÄ Running experiment: {experiment_id}\")\n",
    "    run_result = run_experiment(experiment_id)\n",
    "    \n",
    "    if run_result:\n",
    "        print(\"‚úÖ Experiment completed successfully!\")\n",
    "        \n",
    "        # Run evaluations\n",
    "        print(\"üìä Running tool call accuracy evaluation...\")\n",
    "        eval_result = run_experiment_evals(experiment_id, ['tool_call_accuracy'])\n",
    "        \n",
    "        if eval_result:\n",
    "            print(\"‚úÖ Evaluation completed!\")\n",
    "            \n",
    "            # Extract and display results\n",
    "            print(\"\\nüìà Results Summary:\")\n",
    "            for row in eval_result.get('rows', []):\n",
    "                row_input = row.get('input', {})\n",
    "                print(f\"\\nTest Case: {row_input.get('name', 'Unknown')} - {row_input.get('category', 'Unknown')}\")\n",
    "                \n",
    "                for result in row.get('results', []):\n",
    "                    column_name = result.get('column_name', 'Unknown Column')\n",
    "                    eval_results = result.get('evaluation_result', {})\n",
    "                    \n",
    "                    for evaluator_name, eval_data in eval_results.items():\n",
    "                        score = eval_data.get('results', {}).get('primary_score', 'N/A')\n",
    "                        print(f\"  {column_name}: {score}/1.0\")\n",
    "                        \n",
    "        else:\n",
    "            print(\"‚ùå Evaluation failed\")\n",
    "    else:\n",
    "        print(\"‚ùå Experiment run failed\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No experiment to run - create one first\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e4be5e",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated the complete KeywordsAI evaluation workflow:\n",
    "\n",
    "### ‚úÖ Completed Steps:\n",
    "1. **Agent Demo** - Travel assistant with multi-modal inputs and tool calls\n",
    "2. **Log Management** - Fetched logs with evaluation identifiers\n",
    "3. **Evaluator Creation** - Custom LLM evaluator for tool call accuracy\n",
    "4. **Testset Management** - Created structured test scenarios\n",
    "5. **Experiment Creation** - Used prompts API for proper prompt management\n",
    "6. **Evaluation Execution** - Ran experiments and applied evaluators\n",
    "\n",
    "### üéØ Key Benefits:\n",
    "- **Systematic Evaluation** - Structured approach to LLM agent testing\n",
    "- **Prompt Management** - Proper versioning and comparison of prompts\n",
    "- **Multi-modal Support** - Text + image inputs for comprehensive testing\n",
    "- **Tool Call Assessment** - Specific evaluation of agent tool usage\n",
    "- **Scalable Framework** - Easy to add more evaluators and test cases\n",
    "\n",
    "### üîÑ Next Steps:\n",
    "- Add more evaluators (response quality, safety, etc.)\n",
    "- Create larger testsets with diverse scenarios\n",
    "- Compare different models and temperatures\n",
    "- Set up automated evaluation pipelines\n",
    "- Integrate with CI/CD for continuous evaluation\n",
    "\n",
    "The workflow is now ready for production use with KeywordsAI's evaluation platform!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
